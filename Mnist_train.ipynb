{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c858c8",
   "metadata": {},
   "source": [
    "## 深度學習系列| 解讀LeNet及PyTorch實現\n",
    "###  from [CSDN](https://blog.csdn.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e9e97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeModel(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (FC1): Linear(in_features=640, out_features=64, bias=True)\n",
      "  (FC2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LeModel(nn.Module):\n",
    "    def __init__(self, num_class=10):\n",
    "        super(LeModel, self).__init__()\n",
    "        # CONV2d(in_ch, out_ch, k_size)\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)   # 1x28x28 -> 20x24x24\n",
    "        self.pool1 = nn.MaxPool2d(2)    # 20x24x24 -> 20x12x12\n",
    "        self.conv2 = nn.Conv2d(20, 40, 5)    # 20x12x12 -> 40x8x8\n",
    "        self.pool2 = nn.MaxPool2d(2)    # 16x8x8 -> 40x4x4\n",
    "        self.FC1 = nn.Linear(640*1*1, 64)\n",
    "        self.FC2 = nn.Linear(64, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "    \n",
    "        \n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        #x = torch.tanh(self.conv3(x))\n",
    "        x = x.view(-1, 640*1*1)\n",
    "        x = self.FC1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.FC2(x)\n",
    "        return x\n",
    "    \n",
    "model = LeModel(num_class=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d61af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "\n",
    "# torchsummary.summary(model, input_size = (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f311ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n",
      "3750\n",
      "625\n",
      "epoch: 1 | step:   500 | train_loss: 2.20033 |\n",
      "epoch: 1 | step:  1000 | train_loss: 1.29783 |\n",
      "epoch: 1 | step:  1500 | train_loss: 0.53149 |\n",
      "epoch: 1 | step:  2000 | train_loss: 0.42091 |\n",
      "epoch: 1 | step:  2500 | train_loss: 0.32488 |\n",
      "epoch: 1 | step:  3000 | train_loss: 0.29735 |\n",
      "epoch: 1 | step:  3500 | train_loss: 0.26630 |\n",
      "epoch: 2 | step:   500 | train_loss: 0.21664 |\n",
      "epoch: 2 | step:  1000 | train_loss: 0.22269 |\n",
      "epoch: 2 | step:  1500 | train_loss: 0.18310 |\n",
      "epoch: 2 | step:  2000 | train_loss: 0.18658 |\n",
      "epoch: 2 | step:  2500 | train_loss: 0.15261 |\n",
      "epoch: 2 | step:  3000 | train_loss: 0.16178 |\n",
      "epoch: 2 | step:  3500 | train_loss: 0.15172 |\n",
      "epoch: 3 | step:   500 | train_loss: 0.12899 |\n",
      "epoch: 3 | step:  1000 | train_loss: 0.13624 |\n",
      "epoch: 3 | step:  1500 | train_loss: 0.11543 |\n",
      "epoch: 3 | step:  2000 | train_loss: 0.12134 |\n",
      "epoch: 3 | step:  2500 | train_loss: 0.10465 |\n",
      "epoch: 3 | step:  3000 | train_loss: 0.11568 |\n",
      "epoch: 3 | step:  3500 | train_loss: 0.11050 |\n",
      "epoch: 4 | step:   500 | train_loss: 0.09643 |\n",
      "epoch: 4 | step:  1000 | train_loss: 0.10072 |\n",
      "epoch: 4 | step:  1500 | train_loss: 0.08659 |\n",
      "epoch: 4 | step:  2000 | train_loss: 0.09347 |\n",
      "epoch: 4 | step:  2500 | train_loss: 0.08338 |\n",
      "epoch: 4 | step:  3000 | train_loss: 0.09283 |\n",
      "epoch: 4 | step:  3500 | train_loss: 0.08940 |\n",
      "epoch: 5 | step:   500 | train_loss: 0.08029 |\n",
      "epoch: 5 | step:  1000 | train_loss: 0.08146 |\n",
      "epoch: 5 | step:  1500 | train_loss: 0.07121 |\n",
      "epoch: 5 | step:  2000 | train_loss: 0.07814 |\n",
      "epoch: 5 | step:  2500 | train_loss: 0.07138 |\n",
      "epoch: 5 | step:  3000 | train_loss: 0.07929 |\n",
      "epoch: 5 | step:  3500 | train_loss: 0.07651 |\n",
      "epoch: 6 | step:   500 | train_loss: 0.07047 |\n",
      "epoch: 6 | step:  1000 | train_loss: 0.06905 |\n",
      "epoch: 6 | step:  1500 | train_loss: 0.06182 |\n",
      "epoch: 6 | step:  2000 | train_loss: 0.06829 |\n",
      "epoch: 6 | step:  2500 | train_loss: 0.06324 |\n",
      "epoch: 6 | step:  3000 | train_loss: 0.07002 |\n",
      "epoch: 6 | step:  3500 | train_loss: 0.06767 |\n",
      "epoch: 7 | step:   500 | train_loss: 0.06349 |\n",
      "epoch: 7 | step:  1000 | train_loss: 0.06024 |\n",
      "epoch: 7 | step:  1500 | train_loss: 0.05545 |\n",
      "epoch: 7 | step:  2000 | train_loss: 0.06109 |\n",
      "epoch: 7 | step:  2500 | train_loss: 0.05748 |\n",
      "epoch: 7 | step:  3000 | train_loss: 0.06327 |\n",
      "epoch: 7 | step:  3500 | train_loss: 0.06100 |\n",
      "epoch: 8 | step:   500 | train_loss: 0.05840 |\n",
      "epoch: 8 | step:  1000 | train_loss: 0.05354 |\n",
      "epoch: 8 | step:  1500 | train_loss: 0.05064 |\n",
      "epoch: 8 | step:  2000 | train_loss: 0.05539 |\n",
      "epoch: 8 | step:  2500 | train_loss: 0.05285 |\n",
      "epoch: 8 | step:  3000 | train_loss: 0.05783 |\n",
      "epoch: 8 | step:  3500 | train_loss: 0.05572 |\n",
      "epoch: 9 | step:   500 | train_loss: 0.05421 |\n",
      "epoch: 9 | step:  1000 | train_loss: 0.04813 |\n",
      "epoch: 9 | step:  1500 | train_loss: 0.04674 |\n",
      "epoch: 9 | step:  2000 | train_loss: 0.05059 |\n",
      "epoch: 9 | step:  2500 | train_loss: 0.04914 |\n",
      "epoch: 9 | step:  3000 | train_loss: 0.05360 |\n",
      "epoch: 9 | step:  3500 | train_loss: 0.05124 |\n",
      "epoch: 10 | step:   500 | train_loss: 0.05074 |\n",
      "epoch: 10 | step:  1000 | train_loss: 0.04369 |\n",
      "epoch: 10 | step:  1500 | train_loss: 0.04368 |\n",
      "epoch: 10 | step:  2000 | train_loss: 0.04668 |\n",
      "epoch: 10 | step:  2500 | train_loss: 0.04614 |\n",
      "epoch: 10 | step:  3000 | train_loss: 0.04999 |\n",
      "epoch: 10 | step:  3500 | train_loss: 0.04752 |\n",
      "Finished Training !\n",
      "Accuracy of the network on the 10000 test images: 98 %\n",
      "correct =  9818\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "#from LeNet import LeModel\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "#def main():\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(f'device is {device}')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_data = MNIST(root='./torch_v2', train=True,\n",
    "                   transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=False)\n",
    "\n",
    "test_data = MNIST(root='./torch_v2', train=False,\n",
    "                  transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "#------------------\n",
    "\n",
    "i = 0\n",
    "for images, labels in train_loader:\n",
    "    i = i + 1\n",
    "print(i)\n",
    "i = 0\n",
    "for images, labels in test_loader:\n",
    "    i = i + 1\n",
    "print(i)\n",
    "\n",
    "net = LeModel()\n",
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.5)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "## train ##\n",
    "net.train()\n",
    "# .train 是用來啟動 batch norm & drop out\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for step, (images, labels) in enumerate(train_loader, start=0):\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = net(images)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if step % 500 == 499:\n",
    "            print('epoch: %d | step: %5d | train_loss: %.5f |' % (epoch+1, step+1, running_loss/500))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training !')\n",
    "\n",
    "## evaluate ##\n",
    "net.eval()\n",
    "# .eval 是用來關閉 batch norm & drop out\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = net(images)\n",
    "        _, predict = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum().item()\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "print(\"correct = \", correct)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00176766",
   "metadata": {},
   "source": [
    "### Print test input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536ee811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size =  torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "global img0\n",
    "for images0, label0 in test_loader:\n",
    "    if index > 0:\n",
    "        break\n",
    "    #print(images0[0])\n",
    "    img0 = images0[0]\n",
    "    index = index + 1\n",
    "print(\"input size = \", img0.shape)\n",
    "\n",
    "f = open(\"./para/input.txt\", 'w')\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        print(\"%f,\" %float(img0[0][i][j]), end = '', file = f)\n",
    "    print(\"\",file = f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35874f8a",
   "metadata": {},
   "source": [
    "### Show feature map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8363b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "20\n",
      "24\n",
      "24\n",
      "output channel = 0\n"
     ]
    }
   ],
   "source": [
    "## print layer intermediate output\n",
    "# exact_list = ['conv1']\n",
    "# myexactor = FeatureExtractor(net, exact_list)  # 输出是一个网络\n",
    "\n",
    "\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "net.conv1.register_forward_hook(get_features(\"conv1\"))\n",
    "#net.FC2.register_forward_hook(get_features(\"FC2\"))\n",
    "\n",
    "# placeholders\n",
    "PREDS = []\n",
    "FEATS = []\n",
    "\n",
    "# placeholder for batch features\n",
    "features = {}\n",
    "#net = net.to(device)\n",
    "img0 = img0.to(device)\n",
    "\n",
    "preds = net(img0)\n",
    "\n",
    "PREDS.append(preds.detach().cpu().numpy())\n",
    "FEATS.append(features[\"conv1\"].cpu().numpy())\n",
    "\n",
    "print(len(FEATS))\n",
    "print(len(FEATS[0]))\n",
    "print(len(FEATS[0][0]))\n",
    "print(len(FEATS[0][0][0]))\n",
    "\n",
    "\n",
    "    \n",
    "f = open('./para/conv1_output.txt', 'w')\n",
    "for out_ch in range(1):\n",
    "    print(\"output channel =\", out_ch)\n",
    "    for row in range (24):\n",
    "        for col in range(24):\n",
    "            print(\"%f,\" % (float(FEATS[0][out_ch][row][col])),end='',file = f) \n",
    "            #print(\"%f,\" % (float(FEATS[0][out_ch][row][col])),end='') \n",
    "        print(\"\\n\\n\",file = f)\n",
    "f.close()\n",
    "    \n",
    "# net.FC2.register_forward_hook(get_features(\"FC2\"))\n",
    "# FEATS.append(features[\"FC2\"].cpu().numpy())\n",
    "# print(len(FEATS))\n",
    "# print(len(FEATS[1]))\n",
    "# print(FEATS[1])\n",
    "\n",
    "# x = myexactor(img0)\n",
    "\n",
    "# out = net.print_layer(img0)\n",
    "# print(\"output size = \", out.shape)\n",
    "# #print(output.type)\n",
    "# #print(output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb452199",
   "metadata": {},
   "source": [
    "## print model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c044ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'FC1.weight', 'FC1.bias', 'FC2.weight', 'FC2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# for param in model.parameters():\n",
    "#     print((param.data))\n",
    "or_dict = net.state_dict()\n",
    "#print(float(or_dict['conv1.weight'][0][0][0][0]))\n",
    "#print(model.state_dict())\n",
    "# for key, value in or_dict.iteritems() :\n",
    "#     print(key)\n",
    "print(or_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a514e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel 1: \n",
      "tensor([[ 0.1588, -0.1289,  0.2158,  0.2100,  0.2369],\n",
      "        [-0.0027, -0.2277,  0.2486,  0.4137,  0.2132],\n",
      "        [-0.0897, -0.1108,  0.0621,  0.4653,  0.4454],\n",
      "        [-0.1639,  0.0008,  0.3310,  0.4311,  0.1824],\n",
      "        [ 0.0968, -0.0749,  0.2344,  0.1288, -0.1309]], device='cuda:0')\n",
      "bias 1: \n",
      "tensor(0.1770, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"kernel 1: \")\n",
    "print((or_dict['conv1.weight'][0][0]))\n",
    "print(\"bias 1: \")\n",
    "print((or_dict['conv1.bias'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8169b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#   print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1a89ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.158808,-0.128901,0.215808,0.209981,0.236942,-0.002693,-0.227722,0.248634,0.413741,0.213184,-0.089750,-0.110789,0.062129,0.465253,0.445366,-0.163879,0.000763,0.331031,0.431078,0.182399,0.096818,-0.074894,0.234397,0.128761,-0.130873,"
     ]
    }
   ],
   "source": [
    "f = open('./para/conv1_weight.txt', 'w')\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(\"%f,\" % (or_dict['conv1.weight'][0][0][i][j]),end='',file = f) \n",
    "        print(\"%f,\" % (or_dict['conv1.weight'][0][0][i][j]),end='')\n",
    "    print(\"\\n\", file = f)\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd3d7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.0630,  -2.0820,   3.9497,   5.1293,  -5.6467,  -2.6576, -14.9247,\n",
       "          13.5624,  -2.1464,   2.0591]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(img0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1485bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = './MNIST/model/model_state_dict.pt'\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "852091b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb85a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7419710",
   "metadata": {},
   "source": [
    "### print model weight\n",
    "#### pytorch weight shape : out_ch, in_ch, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a76fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "layer shape =  torch.Size([20, 1, 5, 5])\n",
      "conv1.bias\n",
      "layer shape =  torch.Size([20])\n",
      "conv2.weight\n",
      "layer shape =  torch.Size([40, 20, 5, 5])\n",
      "conv2.bias\n",
      "layer shape =  torch.Size([40])\n",
      "FC1.weight\n",
      "layer shape =  torch.Size([64, 640])\n",
      "FC1.bias\n",
      "layer shape =  torch.Size([64])\n",
      "FC2.weight\n",
      "layer shape =  torch.Size([10, 64])\n",
      "FC2.bias\n",
      "layer shape =  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./para/para.h\", 'w')\n",
    "conv_index = 1\n",
    "fc_index = 1\n",
    "print(\"#include \\\"../../inference/nn.h\\\"\\n\\n\",file = f)\n",
    "for layer_name in or_dict.keys():\n",
    "    if 'conv' in layer_name and 'weight' in layer_name:\n",
    "        print(layer_name)\n",
    "        print(\"layer shape = \", or_dict[layer_name].shape)\n",
    "        print(\"const double conv%d_w_raw[] = {\" %conv_index, file = f)\n",
    "        for out_ch in range (or_dict[layer_name].shape[0]):\n",
    "            for in_ch in range (or_dict[layer_name].shape[1]):\n",
    "                #print(\"%dth kernel, %dth channel\" % (out_ch, in_ch), file = f)\n",
    "                for h in range (or_dict[layer_name].shape[2]):\n",
    "                    for w in range (or_dict[layer_name].shape[3]):\n",
    "                        print(\"(%.15f), \" % (float(or_dict[layer_name][out_ch][in_ch][h][w])),end='',file = f)\n",
    "                    #print (\"\\n\", file = f)\n",
    "                #print(\"----------------\\n\", file = f)\n",
    "        print(\"};\\n\\n\", file = f)\n",
    "    elif 'conv' in layer_name and 'bias' in layer_name:\n",
    "        print(layer_name)\n",
    "        print(\"layer shape = \", or_dict[layer_name].shape)\n",
    "        print(\"const double conv%d_b_raw[] = {\" %conv_index, file = f)\n",
    "        conv_index = conv_index + 1\n",
    "        for ch in range (or_dict[layer_name].shape[0]):\n",
    "            print(\"(%.15f), \" % (float(or_dict[layer_name][ch])),end='',file = f) \n",
    "        print(\"};\\n\\n\", file = f)\n",
    "    elif 'weight' in layer_name: # fc weight\n",
    "        print(layer_name)\n",
    "        print(\"layer shape = \", or_dict[layer_name].shape)\n",
    "        print(\"const double fc%d_w_raw[] = {\" %fc_index, file = f)\n",
    "        for out_ch in range (or_dict[layer_name].shape[0]):\n",
    "            for in_ch in range (or_dict[layer_name].shape[1]):\n",
    "                 print(\"(%.15f), \" % (float(or_dict[layer_name][out_ch][in_ch])),end='',file = f)\n",
    "        print(\"};\\n\\n\", file = f)\n",
    "    elif 'bias' in layer_name: #fc bias\n",
    "        print(layer_name)\n",
    "        print(\"layer shape = \", or_dict[layer_name].shape)\n",
    "        print(\"const double fc%d_b_raw[] = {\" %fc_index, file = f)\n",
    "        fc_index = fc_index + 1\n",
    "        for out_ch in range (or_dict[layer_name].shape[0]):\n",
    "            print(\"(%.15f), \" % (float(or_dict[layer_name][out_ch])),end='',file = f)\n",
    "        print(\"};\\n\\n\", file = f)\n",
    "        \n",
    "f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b088c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_v2",
   "language": "python",
   "name": "pytorch_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
